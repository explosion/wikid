title: 'wikid'
description: |
  [![Azure Pipelines](https://img.shields.io/azure-devops/build/explosion-ai/public/32/main.svg?logo=azure-pipelines&style=flat-square&label=build)](https://dev.azure.com/explosion-ai/public/_build?definitionId=32)
  [![spaCy](https://img.shields.io/static/v1?label=made%20with%20%E2%9D%A4%20and&message=spaCy&color=09a3d5&style=flat-square)](https://spacy.io)
  <br/>
  _No REST for the `wikid`_ :jack_o_lantern: - generate a SQLite database and a spaCy `KnowledgeBase` from Wikipedia & 
  Wikidata dumps. `wikid` was designed with the use case of named entity linking (NEL) with spaCy in mind.
  <br/>
  Note this repository is still in an experimental stage, so the public API might change at any time. 

vars:
  version: "0.0.1"
  language: "en"
  vectors_model: "en_core_web_lg"
  filter: "True"

directories: ["assets", "configs", "scripts", "output"]

assets:
  - dest: 'assets/wikidata_entity_dump.json.bz2'
    url: 'https://dumps.wikimedia.org/wikidatawiki/entities/latest-all.json.bz2'
    description: Wikidata entity dump. Download can take a long time!
    extra: True
  - dest: 'assets/${vars.language}-wikipedia_dump.xml.bz2'
    url: 'https://dumps.wikimedia.org/${vars.language}wiki/latest/${vars.language}wiki-latest-pages-articles-multistream.xml.bz2'
    description: Wikipedia dump. Download can take a long time!
    extra: True
  - dest: 'assets/wikidata_entity_dump_filtered.json.bz2'
    url: 'https://github.com/explosion/projects/releases/download/nel-benchmark-filtered-wiki-data/wikidata_entity_dump_filtered.json.bz2'
    description: Filtered Wikidata entity dump for demo purposes (English only).
    checksum: 'ba2d979105abf174208608b942242fcb'
  - dest: 'assets/wikipedia_dump_filtered.xml.bz2'
    url: 'https://github.com/explosion/projects/releases/download/nel-benchmark-filtered-wiki-data/wikipedia_dump_filtered.xml.bz2'
    description: Filtered Wikipedia dump for demo purposes (English only).
    checksum: 'cb624eaa5887fe1ff47a9206c9bdcfd8'

workflows:
  all:
    - parse
    - download_model
    - create_kb

commands:
  - name: parse
    help: "Parse Wiki dumps. This can take a long time if you're not using the filtered dumps!"
    script:
      - "env PYTHONPATH=scripts python ./scripts/parse.py ${vars.language} ${vars.filter}"
    outputs:
      - "output/${vars.language}/wiki.sqlite3"

  - name: download_model
    help: "Download spaCy language model."
    script:
      - "spacy download ${vars.vectors_model}"

  - name: create_kb
    help: "Creates KB utilizing SQLite database with Wiki content."
    script:
      - "env PYTHONPATH=scripts python ./scripts/create_kb.py ${vars.vectors_model} ${vars.language}"
    deps:
      - "output/${vars.language}/wiki.sqlite3"
    outputs:
      - "output/${vars.language}/kb"
      - "output/${vars.language}/nlp"

  - name: delete_db
    help: "Deletes SQLite database generated in step parse with data parsed from Wikidata and Wikipedia dump."
    script:
      - "rm -f output/${vars.language}/wiki.sqlite3"
    deps:
      - "output/${vars.language}/wiki.sqlite3"

  - name: clean
    help: "Delete all generated artifacts except for SQLite database."
    script:
      - "rm -f output/${vars.language}/kb output/${vars.language}/nlp"
    deps:
      - "output/${vars.language}/kb"
      - "output/${vars.language}/nlp"
